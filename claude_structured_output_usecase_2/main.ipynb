{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57f7846-1ccf-4b5d-bfd0-88120c578466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import dotenv\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7047bd83-b48c-4298-a727-1452680fe65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 1.40.21\n",
      "pandas 2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"boto3\", boto3.__version__)\n",
    "print(\"pandas\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a87861-aa01-4466-bcef-46708622b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "CLAUDE_MODEL_ID = os.getenv(\"BEDROCK_CLAUDE_MODEL_ID\")\n",
    "CLAUDE_CLIENT = boto3.client(\"bedrock-runtime\", region_name=\"ap-northeast-1\")\n",
    "FUNCTION_CALLING_NAME = \"hogehoge\"\n",
    "FUNCTION_CALLING_DESCRIPTION = \"\"\"\n",
    "あなたはコンタクトセンターで音声通話テキストからお客様のニーズ(VOC)を把握する業務を10年以上行っている専門家です。\n",
    "与えられたテキストは過去の通話を記録したもので、CUはお客様を表し、OPはオペレータの発言になります。\n",
    "\"\"\"\n",
    "INPUT_DATA_PATH = \"複数行1列の形で音声通話テキストが保存されているcsvファイル\"\n",
    "OUTPUT_FOLDER = \"出力ファイルの保存先のディレクトリ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167baaa-be83-499d-8e46-648cfb34b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_1_IN_FUNCTION_CALLING = \"predict_customer_request_by_LLM\"\n",
    "PROMPT_1_IN_FUNCTION_CALLING = \"会話はコンタクトセンターでの電話のやり取りです。このやり取りの中から、CUの要望を推測して、時系列順に箇条書きのリストにする事。\"\n",
    "NAME_2_IN_FUNCTION_CALLING = \"judge_achieve_customer_request_by_LLM\"\n",
    "PROMPT_2_IN_FUNCTION_CALLING = \"お客様の{a}のそれぞれの要望が満たされたかどうかを、要望毎に推測してリストにする事。\".format(a=NAME_1_IN_FUNCTION_CALLING)\n",
    "NAME_3_IN_FUNCTION_CALLING = \"reason_of_achievement_judge_by_LLM\"\n",
    "PROMPT_3_IN_FUNCTION_CALLING = \"{a}のそれぞれの評価になった理由を、要望毎に説明してリストにする事。\".format(a=NAME_2_IN_FUNCTION_CALLING)\n",
    "NAME_4_IN_FUNCTION_CALLING = \"judge_customer_satisfaction_score_by_LLM\"\n",
    "PROMPT_4_IN_FUNCTION_CALLING = \"オペレータの応対に対してのお客様の満足度を推測して数値で表すこと。不満足ならば0、満足ならば100とする。(0-100)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568fd400-120e-4c06-b33b-af7246c281f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_output_tool():\n",
    "    return {\"toolSpec\": {\"name\": FUNCTION_CALLING_NAME,\n",
    "                         \"description\": FUNCTION_CALLING_DESCRIPTION,\n",
    "                         \"inputSchema\": {\"json\": {\"type\": \"object\",\n",
    "                                                  \"properties\": {NAME_1_IN_FUNCTION_CALLING: {\"type\": \"array\",\n",
    "                                                                                              \"items\": {\"type\": \"string\"},\n",
    "                                                                                              \"description\": PROMPT_1_IN_FUNCTION_CALLING},\n",
    "                                                                 NAME_2_IN_FUNCTION_CALLING: {\"type\": \"array\",\n",
    "                                                                                              \"items\": {\"type\": \"string\"},\n",
    "                                                                                              \"description\": PROMPT_2_IN_FUNCTION_CALLING,\n",
    "                                                                                              \"enum\": [\"True\", \"False\"]},\n",
    "                                                                 NAME_3_IN_FUNCTION_CALLING: {\"type\": \"array\",\n",
    "                                                                                              \"items\": {\"type\": \"string\"},\n",
    "                                                                                              \"description\": PROMPT_3_IN_FUNCTION_CALLING},\n",
    "                                                                 NAME_4_IN_FUNCTION_CALLING: {\"type\": \"number\",\n",
    "                                                                                              \"description\": PROMPT_4_IN_FUNCTION_CALLING}\n",
    "                                                                },\n",
    "                                                  \"required\": [NAME_1_IN_FUNCTION_CALLING,\n",
    "                                                               NAME_2_IN_FUNCTION_CALLING,\n",
    "                                                               NAME_3_IN_FUNCTION_CALLING,\n",
    "                                                               NAME_4_IN_FUNCTION_CALLING]\n",
    "                                                 }\n",
    "                                        }\n",
    "                        }\n",
    "           }\n",
    "\n",
    "\n",
    "def process_claude_with_function_calling(call_text_and_speaker):\n",
    "    prompt = \"\"\"\n",
    "    <text>\n",
    "    {a}\n",
    "    </text>\n",
    "    \"\"\".format(a=call_text_and_speaker)\n",
    "    claude_input_with_prompt = [{\"role\": \"user\",\n",
    "                                 \"content\": [{\"text\": prompt}]\n",
    "                                }]\n",
    "    claude_response = CLAUDE_CLIENT.converse(modelId=CLAUDE_MODEL_ID,\n",
    "                                             messages=claude_input_with_prompt,\n",
    "                                             inferenceConfig={\"temperature\": 0.2},\n",
    "                                             toolConfig={\"tools\": [structured_output_tool()],\n",
    "                                                         \"toolChoice\": {\"tool\": {\"name\": FUNCTION_CALLING_NAME}}\n",
    "                                                        })\n",
    "    result = claude_response[\"output\"][\"message\"][\"content\"][0][\"toolUse\"][\"input\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24b400-fb7a-45dd-86c0-13ffd72d30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file_and_inference_using_claude(csv_file_path):\n",
    "    df = pd.DataFrame()\n",
    "    with open(file=csv_file_path, mode=\"r\", encoding=\"utf-8\") as csvfile:\n",
    "        csvfile_data_reader = csv.reader(csvfile)\n",
    "        next(csvfile_data_reader, None)  # 1行目のヘッダーをスキップする\n",
    "        print(datetime.datetime.now())\n",
    "        conversation_id = 0  # 便宜上の会話ID\n",
    "        for row in csvfile_data_reader:\n",
    "            if row:\n",
    "                conversation_id += 1\n",
    "                predict_result = process_claude_with_function_calling(call_text_and_speaker=row[0])  # 変数rowがlist型だったので[0]でstringを取得\n",
    "                dict_for_dataframe = {\"conversation_ID\": conversation_id,  # 数値\n",
    "                                      \"comment\": row[0],  # 文字列\n",
    "                                      \"customer_request\": predict_result[\"predict_customer_request_by_LLM\"],  # リスト\n",
    "                                      \"judge_achievement\": predict_result[\"judge_achieve_customer_request_by_LLM\"],  # リスト\n",
    "                                      \"reason_of_achievement\": predict_result[\"reason_of_achievement_judge_by_LLM\"],  # リスト\n",
    "                                      \"satisfaction_score\": predict_result[\"judge_customer_satisfaction_score_by_LLM\"]}  # 数値\n",
    "                try:\n",
    "                    temp_df = pd.DataFrame(data=dict_for_dataframe)\n",
    "                    temp_df = temp_df.explode([\"customer_request\", \"judge_achievement\", \"reason_of_achievement\"])\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "                df = pd.concat(objs=[df, temp_df], axis=0)\n",
    "                time.sleep(1)  # bedrockのapiリクエスト制限を防ぐための待ち\n",
    "            else:\n",
    "                continue\n",
    "        print(datetime.datetime.now())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91731fc-5831-4ffd-a818-127cfc5064af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv_file_and_inference_using_claude(csv_file_path=INPUT_DATA_PATH)\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "df.to_csv(path_or_buf=\"{a}/hogehoge_result.csv\".format(a=OUTPUT_FOLDER),\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tanaka_kernel(Python3-11)",
   "language": "python",
   "name": "tanaka_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
