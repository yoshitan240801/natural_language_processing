# このフォルダのプログラムについて

このフォルダのmainプログラム(main.ipynb)は、Hugging Faceのtransformersライブラリーの勉強を兼ねて、東北大のBERTモデルにて、Fine Tuning前での文章分類の精度と、Fine Tuning後での文章分類の精度を比較してみたものになります。<br>
&nbsp;(numpy： 2.0.2)<br>
&nbsp;(pandas： 2.2.2)<br>
&nbsp;(torch： 2.6.0+cu124)<br>
&nbsp;(transformers： 4.50.2)<br>
&nbsp;(datasets： 3.5.0)<br>


# 補足(BERTのデータ形状(shape)の流れ)

## 【前提】

- **ミニバッチ処理**を行う。
- **文章分類タスク**を想定。
  - 時系列数とは、トークン数のこと。
- **埋め込み次元数**とは、例えば768などのEmbeddingの次元数を指す。

---

## 入力〜出力のデータフロー

```mermaid
graph TB
    A[入力データ<br>ミニバッチ数, 時系列数, 埋め込み次元数] --> 
    B[Positional Encoding層<br>ミニバッチ数, 時系列数, 埋め込み次元数]
    B --> C[Multihead Self Attention層<br>ミニバッチ数, 時系列数, 埋め込み次元数]
    C --> D[FeedForward層<br>ミニバッチ数, 時系列数, 埋め込み次元数]
    D --> E[プーリング層<br>ミニバッチ数, 埋め込み次元数]
    E --> F[出力層<br>ミニバッチ数, 出力層のパーセプトロン数]
    F --> G[出力データ]
```

---

## ポイント

**プーリング層**では、`[CLS]`トークンの埋め込みベクトルを抽出する処理を行う。<br>
通常、時系列の最初の位置（0番目）には `[CLS]` トークンが配置されているので、これに対応するベクトルを抽出する。<br>
具体的には、出力テンソルの形状 `(ミニバッチ数, 時系列数, 埋め込み次元数)` から、`[CLS]` トークンに該当する部分だけを取り出す事で、<br>
 `(ミニバッチ数, 1, 埋め込み次元数)` になるので、これをsqueezeする事で、 `(ミニバッチ数, 埋め込み次元数)` になる。<br>
このテンソルが出力的な文章表現として、出力層に渡される。
